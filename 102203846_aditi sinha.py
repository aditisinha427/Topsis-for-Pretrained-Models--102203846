# -*- coding: utf-8 -*-
"""102203846_Aditi Sinha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YkT-HIHcMkcotmNOKeM0fLiaUZLhIGGB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = {
    'Model': ['GPT-3', 'GPT-4', 'LLaMA'],
    'Perplexity': [20, 15, 18],
    'BLEU': [0.7, 0.8, 0.75],
    'ROUGE': [0.6, 0.65, 0.62],
    'Latency': [1.5, 2.0, 1.8],
    'Memory': [8, 12, 10]
}

df = pd.DataFrame(data)

df.head()

# Step 1: Normalize the data
criteria = ['Perplexity', 'BLEU', 'ROUGE', 'Latency', 'Memory']
beneficial = ['BLEU', 'ROUGE']
non_beneficial = ['Perplexity', 'Latency', 'Memory']

# Step 1: Normalize the data
criteria = ['Perplexity', 'BLEU', 'ROUGE', 'Latency', 'Memory']
beneficial = ['BLEU', 'ROUGE']
non_beneficial = ['Perplexity', 'Latency', 'Memory']

for crit in criteria:
    if crit in beneficial:
        df[crit] = (df[crit] - df[crit].min()) / (df[crit].max() - df[crit].min())
    else:
        df[crit] = (df[crit].max() - df[crit]) / (df[crit].max() - df[crit].min())

# Step 2: Assign weights
weights = [0.3, 0.25, 0.25, 0.1, 0.1]

# Step 3: Weighted normalized decision matrix
for i, crit in enumerate(criteria):
    df[crit] = df[crit] * weights[i]

# Step 4: Calculate PIS and NIS
pis = df[criteria].max()
nis = df[criteria].min()

# Step 5: Euclidean distances
df['D+'] = np.sqrt(((df[criteria] - pis) ** 2).sum(axis=1))
df['D-'] = np.sqrt(((df[criteria] - nis) ** 2).sum(axis=1))

# Step 6: Calculate relative closeness
df['C'] = df['D-'] / (df['D+'] + df['D-'])

# Step 7: Rank models
df['Rank'] = df['C'].rank(ascending=False)

# Results
print(df[['Model', 'C', 'Rank']])

# Visualization
plt.bar(df['Model'], df['C'])
plt.title('Model Ranking Using TOPSIS')
plt.xlabel('Model')
plt.ylabel('Relative Closeness')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

normalized_data = df[criteria]

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(normalized_data, annot=True, cmap='coolwarm', xticklabels=criteria, yticklabels=df['Model'])
plt.title('Heatmap of Normalized Criteria')
plt.xlabel('Criteria')
plt.ylabel('Models')
plt.show()

from math import pi

categories = criteria
num_vars = len(categories)

values = normalized_data.iloc[0].tolist() + [normalized_data.iloc[0, 0]]
angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)] + [0]

plt.figure(figsize=(8, 8))
for i, row in normalized_data.iterrows():
    values = row.tolist() + [row[0]]  # Close the circle
    angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)] + [0]
    plt.polar(angles, values, label=df['Model'][i])

plt.xticks(angles[:-1], categories, color='black', size=10)
plt.fill(angles, values, alpha=0.1)  # Fill area
plt.title('Radar Chart of Models Performance')
plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))
plt.show()

weighted_contributions = df[criteria].mean(axis=0)


plt.figure(figsize=(8, 6))
plt.bar(weighted_contributions.index, weighted_contributions, color='skyblue')
plt.title('Contribution of Each Criterion to Rankings')
plt.xlabel('Criteria')
plt.ylabel('Average Weighted Score')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df['C'], df['Rank'], color='green', edgecolor='black', s=100)
plt.title('Closeness Score vs Ranking')
plt.xlabel('Closeness Score (C)')
plt.ylabel('Rank')
plt.gca().invert_yaxis()  # Invert rank for visual clarity
for i, txt in enumerate(df['Model']):
    plt.annotate(txt, (df['C'][i], df['Rank'][i]), textcoords="offset points", xytext=(0,10), ha='center')
plt.grid()
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(data=normalized_data)
plt.title('Spread of Normalized Criteria Values')
plt.xlabel('Criteria')
plt.ylabel('Normalized Value')
plt.xticks(rotation=45)
plt.show()

sns.pairplot(normalized_data, diag_kind='kde', markers='o')
plt.suptitle('Pairwise Scatter Plots of Normalized Criteria', y=1.02)
plt.show()

plt.savefig('results/visualizations/<plot_name>.png', dpi=300, bbox_inches='tight')